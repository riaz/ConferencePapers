{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://iccv2023.thecvf.com/main.conference.program-107.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "for table in soup.find_all(\"table\", {'class': 'Posters'}):\n",
    "    df = pd.read_html(str(table))[0]\n",
    "    #display(df)\n",
    "    titles = df[['Session','Paper Title Author路s Name路s']].groupby('Session')\n",
    "\n",
    "    for name, group in titles:\n",
    "        if name == \"Human pose/shape estimation\":\n",
    "            group.to_csv(f\"data/Human Pose.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Papers and create JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def save_file(link):\n",
    "    # Saving the PDF File\n",
    "    local_file_name = \"papers/\" + link['href'].strip().split(\"/\")[-1]\n",
    "    uri = \"https://openaccess.thecvf.com/\" + link['href']\n",
    "    r = requests.get(uri)\n",
    "    with open(local_file_name, \"wb\") as code:\n",
    "        code.write(r.content)\n",
    "    return local_file_name\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    paper = {}\n",
    "    conference = \"ICCV2023\"\n",
    "    res = requests.get(\"http://openaccess.thecvf.com/\"+conference+\"?day=all\")\n",
    "    if res.status_code == 200:\n",
    "        soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "        results = soup.find_all(\"dt\", class_=\"ptitle\")\n",
    "        for res in results:\n",
    "            print(res.text)\n",
    "            paper[res.text] = {}\n",
    "            anchor = res\n",
    "            authors = anchor.find_next_sibling(\"dd\")\n",
    "            paper[res.text][\"authors\"] = []\n",
    "            for auth in authors.find_all(\"a\"):\n",
    "                #print(auth.text)\n",
    "                paper[res.text][\"authors\"].append(auth.text)\n",
    "            links = authors.find_next_sibling(\"dd\")\n",
    "            for link in links.find_all(\"a\", string=\"pdf\", href=True):\n",
    "                local_save_location = save_file(link)\n",
    "                paper[res.text][\"url\"] = local_save_location\n",
    "                #print(link['href'])\n",
    "\n",
    "    with open(\"cvf_data_w_pdf.json\", \"w\") as fw:\n",
    "        json.dump(paper, fw, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Above JSON file and generate embedding for the paper titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"cvf_data_w_pdf.json\", \"r\") as fr:\n",
    "    data = json.load(fr)\n",
    "    sentences = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2161, 384)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We will index and save these embeddings to a file\n",
    "import hnswlib\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "dim = embeddings.shape[1]\n",
    "num_elements = len(embeddings)\n",
    "\n",
    "# Generating sample data\n",
    "ids = np.arange(num_elements)\n",
    "\n",
    "# Declaring index\n",
    "p = hnswlib.Index(space = 'l2', dim = dim) # possible options are l2, cosine or ip\n",
    "\n",
    "# Initializing index - the maximum number of elements should be known beforehand\n",
    "p.init_index(max_elements = num_elements, ef_construction = 200, M = 16)\n",
    "\n",
    "# Element insertion (can be called several times):\n",
    "p.add_items(embeddings, ids)\n",
    "\n",
    "# Controlling the recall by setting ef:\n",
    "p.set_ef(50) # ef should always be > k\n",
    "\n",
    "with open(\"embed_model.pkl\", \"wb\") as fw:\n",
    "    pickle.dump(p, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a query engine\n",
    "\n",
    "def get_best_matches(query, k=1):\n",
    "    #p = open(\"embed_model.pkl\",\"rb\")    \n",
    "    query_embedding = model.encode([query])[0]\n",
    "    idx, distances = p.knn_query(query_embedding, k=k)\n",
    "    for label, dist in zip(idx, distances):\n",
    "        return sentences[label[0]]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Few-Shot Common Action Localization via Cross-Attentional Fusion of Context and Temporal Dynamics'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_matches(\"Few-Shot Common Action Localization via Cross-Attentional Fusion of Context and Temporal Dynamics Juntae Lee, Mihir Jain, Sungrack Yun Paper ID:9014 9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Updating this file with closest match\n",
    "import json\n",
    "\n",
    "with open(\"cvf_data_w_pdf.json\", \"r\") as fr:\n",
    "    paper_to_obj = json.load(fr)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "meta_json = defaultdict(list)\n",
    "\n",
    "for fname in glob.glob(\"data/*.csv\"):\n",
    "    with open(fname) as f:\n",
    "        reader = csv.DictReader(f, delimiter=\",\")\n",
    "        category = fname.split(\"/\")[-1].replace('.csv', '')\n",
    "        for row in reader:\n",
    "            paper_name = row[\"Paper Title Author路s Name路s\"]\n",
    "            # find the closest paper_name match            \n",
    "            match = get_best_matches(paper_name)\n",
    "\n",
    "            if match:\n",
    "                meta_json[category].append({\n",
    "                    \"link\": paper_to_obj[match][\"url\"],\n",
    "                    \"title\": match,\n",
    "                    \"authors\": paper_to_obj[match][\"authors\"]\n",
    "                })\n",
    "\n",
    "            #print(category, '|', paper_name)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting meta json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"iccv_data_w_cat.json\", \"w\") as fw:\n",
    "    json.dump(meta_json, fw, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat, papers in meta_json.items():\n",
    "    print(cat, len(papers))\n",
    "    for paper in papers:\n",
    "        print(paper['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from py_markdown_table.markdown_table import markdown_table\n",
    "\n",
    "### Saving as markdown\n",
    "README = \"README.md\"\n",
    "ICCV_DATA = \"iccv_data_w_cat.json\"\n",
    "\n",
    "with open(ICCV_DATA) as fd, open(README, \"w\") as md:\n",
    "    iccv_data = json.load(fd)\n",
    "    \n",
    "    md.write(\"# ICCV 2023 \\n\")\n",
    "    md.write(\"Papers from ICCV 2023 with categories\")\n",
    "\n",
    "    for paper_cat, papers in iccv_data.items():        \n",
    "        \n",
    "        md.write(f\"\\n### {paper_cat} \\n\")\n",
    "\n",
    "        # generating the dataframe\n",
    "        table = []\n",
    "\n",
    "        for paper in papers:\n",
    "            paper_title = paper['title'] \n",
    "            paper_link = paper['link']\n",
    "            md_link = f\"[Paper](<{paper_link}>)\"\n",
    "            table.append([paper_title, md_link])\n",
    "        df = pd.DataFrame(table, columns=[\"Paper Title\", \"Link\"])\n",
    "        content  = markdown_table(df.to_dict(orient='records')).setParams(row_sep = 'markdown', quote = False, padding_weight='centerright').getMarkdown()\n",
    "        md.write(content + \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "applied_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
